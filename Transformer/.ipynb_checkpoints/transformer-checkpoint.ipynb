{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d8b8e31-f45f-4356-b1e9-df7680573a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d3d3655-7712-4a9a-b640-0ca520d59fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffab8a19-d376-4b47-b388-2481f87d4e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFD(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_ffn, input_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.dense1 = Dense(d_ffn, activation='relu')\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.dense2 = Dense(input_size)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a92e11cd-0076-4390-9379-865acc607055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, seq_len, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        position = tf.cast(tf.range(seq_len)[:, tf.newaxis], tf.float32)\n",
    "        div_term = tf.exp(tf.range(0, d_model, 2, dtype=tf.float32) * (-tf.math.log(10000.0) / d_model))\n",
    "\n",
    "        angle_rads = position * div_term\n",
    "        sin_encoding = tf.math.sin(angle_rads)\n",
    "        cos_encoding = tf.math.cos(angle_rads)\n",
    "\n",
    "        pe = tf.stack([sin_encoding, cos_encoding], axis=-1)\n",
    "        pe = tf.reshape(pe, (seq_len, d_model))\n",
    "\n",
    "        self.pos_encoding = tf.expand_dims(pe, axis=0)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = x * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x = x + self.pos_encoding[:, :tf.shape(x)[1], :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "db1d62ca-0773-4389-80ba-0f380679195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        assert d_model % heads == 0, \"d_model mus tbe divisible by nums_heads\"\n",
    "        self.depth = d_model // heads\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        self.wo = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def attention(self,q,k,v,mask=None):\n",
    "        qk = tf.matmul(q,k,transpose_b = True)\n",
    "        dk = tf.cast(tf.shape(k)[-1],tf.float32)\n",
    "        qk = qk/tf.math.sqrt(dk)\n",
    "        if mask is not None:\n",
    "            qk += (mask* -1e9)\n",
    "        weights = tf.nn.softmax(qk,axis=-1)\n",
    "        return tf.matmul(weights,v),weights\n",
    "\n",
    "    def split_heads(self,batch,x):\n",
    "        x = tf.reshape(x,(batch,-1,self.heads,self.depth))\n",
    "        return tf.transpose(x,perm=[0,2,1,3])\n",
    "    def call(self,v,k,q,mask=None):\n",
    "        batch = tf.shape(q)[0]\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        q = self.split_heads(q, batch)\n",
    "        k = self.split_heads(k, batch)\n",
    "        v = self.split_heads(v, batch)\n",
    "\n",
    "        attention_output, attn_weights = self.attention(q, k, v, mask)\n",
    "\n",
    "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention_output, (batch_size, -1, self.d_model))\n",
    "\n",
    "        output = self.wo(concat_attention)\n",
    "\n",
    "        return output, attn_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41cbd7cb-4ea6-4d0c-904c-f97e33dfe9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class normalize(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model, eps:float = 10**-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = self.add_weight(\n",
    "            name=\"gamma\",\n",
    "            shape=(d_model,),\n",
    "            initializer=\"ones\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            name=\"beta\",\n",
    "            shape=(d_model,),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "        variance = tf.reduce_mean(tf.square(x - mean), axis=-1, keepdims=True)\n",
    "        normalized = (x - mean) / tf.sqrt(variance + self.eps)\n",
    "        return self.gamma * normalized + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9f28fa3-fa37-4160-a9b9-1d8e7755ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class residualconnections(tf.keras.layers.Layer):\n",
    "    def __init__(self,d_model,dropout,layer):\n",
    "        super().__init__()\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        self.norm = normalize(d_model)\n",
    "    def call(self,x,sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b848965-6a45-4213-8a85-2c1c7c2f87e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ffn, dropout):\n",
    "        super().__init__()\n",
    "        self.mha = MHA(d_model, num_heads)\n",
    "        self.ffn = FFD(d_ffn, d_model, dropout)\n",
    "\n",
    "        self.res1 = residualconnections(d_model, dropout, self.mha)\n",
    "        self.res2 = residualconnections(d_model, dropout, self.ffn)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        x = self.res1(x, lambda x: self.mha(x, x, x, mask)[0])\n",
    "        x = self.res2(x, self.ffn)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c64f5876-a3f2-45f0-8278-455b70146b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ffn, dropout):\n",
    "        super().__init__()\n",
    "        self.mha1 = MHA(d_model, num_heads)\n",
    "        self.mha2 = MHA(d_model, num_heads)\n",
    "        self.ffn = FFD(d_ffn, d_model, dropout)\n",
    "\n",
    "        self.res1 = residualconnections(d_model, dropout, self.mha1)\n",
    "        self.res2 = residualconnections(d_model, dropout, self.mha2)\n",
    "        self.res3 = residualconnections(d_model, dropout, self.ffn)\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
    "        x = self.res1(x, lambda x: self.mha1(x, x, x, look_ahead_mask)[0])\n",
    "        x = self.res2(x, lambda x: self.mha2(x, enc_output, enc_output, padding_mask)[0])\n",
    "        x = self.res3(x, self.ffn)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3b0fb8d-b6be-4197-abb0-ce882cc4b377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ffn, vocab_size, seq_len, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = InputEmbedding(d_model, vocab_size)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, seq_len, dropout)\n",
    "\n",
    "        self.enc_blocks = [\n",
    "            EncoderBlock(d_model, num_heads, d_ffn, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x, mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c03c0e9b-04c4-4e87-a310-9e4a28f25746",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ffn, vocab_size, seq_len, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = InputEmbedding(d_model, vocab_size)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, seq_len, dropout)\n",
    "\n",
    "        self.dec_blocks = [\n",
    "            DecoderBlock(d_model, num_heads, d_ffn, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "\n",
    "    def call(self, x, enc_output, look_ahead_mask=None, padding_mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        for block in self.dec_blocks:\n",
    "            x = block(x, enc_output, look_ahead_mask, padding_mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0b07fda0-7580-4487-be44-37aa56d42c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        return self.dense(x)  # (batch_size, seq_len, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3c62eaf-38e0-47c4-861c-c9c759c0abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, \n",
    "                 num_layers, d_model, num_heads, d_ffn, \n",
    "                 input_vocab_size, target_vocab_size, \n",
    "                 max_seq_len_input, max_seq_len_target, \n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, d_ffn, input_vocab_size, max_seq_len_input, dropout)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, d_ffn, target_vocab_size, max_seq_len_target, dropout)\n",
    "        \n",
    "        # No final_layer here â€” handled separately by ProjectionLayer\n",
    "        \n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        inp, tar, enc_padding_mask, look_ahead_mask, dec_padding_mask = inputs\n",
    "\n",
    "        enc_output = self.encoder(inp, mask=enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "        dec_output = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)  # (batch_size, tar_seq_len, d_model)\n",
    "        \n",
    "        return dec_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fb961746-40fd-4048-969f-ea792ba64fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(N, d_model, num_heads, d_ffn, \n",
    "                      input_vocab_size, target_vocab_size, \n",
    "                      max_seq_len_input, max_seq_len_target, \n",
    "                      dropout=0.1):\n",
    "    transformer = Transformer(\n",
    "        num_layers=N,               # N stacks of encoder & decoder\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        d_ffn=d_ffn,\n",
    "        input_vocab_size=input_vocab_size,\n",
    "        target_vocab_size=target_vocab_size,\n",
    "        max_seq_len_input=max_seq_len_input,\n",
    "        max_seq_len_target=max_seq_len_target,\n",
    "        dropout=dropout\n",
    "    )\n",
    "    \n",
    "    projection_layer = ProjectionLayer(target_vocab_size)\n",
    "    \n",
    "    return transformer, projection_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "65e96942-abd4-4a60-b98c-6e759835e727",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 6  # Number of encoder & decoder layers\n",
    "\n",
    "transformer, projection = build_transformer(\n",
    "    N=N,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ffn=2048,\n",
    "    input_vocab_size=8500,\n",
    "    target_vocab_size=8000,\n",
    "    max_seq_len_input=100,\n",
    "    max_seq_len_target=100,\n",
    "    dropout=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbb3898-b44c-47bc-a851-557012b50c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
